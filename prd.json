{
  "project": "Chess Speedrun Learning System",
  "branchName": "ralph/verified-puzzles",
  "description": "An adaptive chess tutor system using Claude Code + Stockfish + MCP server + TUI",
  "documentationRef": "documentation/general_chess_speedrun_prd_documentation.md",
  "userStories": [
    {
      "id": "US-001",
      "title": "Create install script and project structure",
      "description": "As a developer, I need the install script, shared data models, and directory structure so the project can be set up.",
      "acceptanceCriteria": [
        "scripts/install.sh exists and is executable",
        "Installs Stockfish via brew (macOS) or apt (Linux); skips if already installed",
        "Installs uv if not present, uses uv for environment management",
        "Installs Python deps via uv: python-chess, rich, textual, watchdog, mcp[cli]",
        "Creates data directories: data/sessions, data/games, data/lesson_plans",
        "Creates data/.gitkeep files in each data subdirectory",
        "Initializes data/progress.json with default values if not exists",
        "Initializes data/srs_cards.json as empty array if not exists",
        "Creates pyproject.toml with project metadata, dependencies (python-chess, rich, textual, watchdog, mcp[cli]), and dev deps (pytest)",
        "scripts/models.py exists with GameState and MoveEvaluation dataclasses",
        "GameState fields: game_id, fen, board_display, move_list, last_move, last_move_san, eval_score, player_color, target_elo, is_game_over, result, legal_moves, accuracy, session_number, streak, lesson_name",
        "MoveEvaluation fields: move_san, best_move_san, cp_loss, eval_before, eval_after, classification, is_best, best_line, tactical_motif",
        "python -m py_compile scripts/models.py passes",
        "python -c \"from scripts.models import GameState, MoveEvaluation\" passes"
      ],
      "priority": 1,
      "passes": true,
      "notes": "Uses uv (not pip/venv) for environment management. models.py is the shared schema for MCP server and TUI."
    },
    {
      "id": "US-002",
      "title": "Implement Stockfish engine wrapper",
      "description": "As a developer, I need a Python wrapper around Stockfish for game play and analysis with adaptive difficulty.",
      "acceptanceCriteria": [
        "scripts/engine.py exists with ChessEngine class",
        "Auto-detects Stockfish binary path (checks /usr/local/bin/stockfish, /usr/bin/stockfish, /opt/homebrew/bin/stockfish, which stockfish)",
        "Raises FileNotFoundError with 'Run scripts/install.sh' message if Stockfish not found",
        "new_game() starts a game with configurable target Elo and player_color",
        "set_difficulty() configures engine strength",
        "Sub-1320 Elo uses linear blend: random_pct = max(0, 0.85 - (elo/1320)*0.85), depth = max(1, min(5, elo//250))",
        "get_engine_move() returns moves at configured difficulty",
        "analyze_position() runs full-strength analysis with multipv support",
        "evaluate_move() compares player move to best move, returns MoveEvaluation from scripts.models",
        "Move classification: 0cp=best, 1-30=great, 31-80=good, 81-150=inaccuracy, 151-300=mistake, 300+=blunder",
        "tactical_motif field set to None (future enhancement)",
        "CLI: python scripts/engine.py analyze <fen> prints top 3 lines",
        "CLI: python scripts/engine.py play <elo> plays one engine move and exits",
        "CLI: python scripts/engine.py play <elo> --interactive runs full game loop",
        "python -m py_compile scripts/engine.py passes",
        "python -c \"from scripts.engine import ChessEngine\" passes",
        "tests/test_engine.py exists with pytest tests",
        "python -m pytest tests/test_engine.py passes"
      ],
      "priority": 2,
      "passes": true,
      "notes": "Core dependency - MCP server depends on this. Tests should mock Stockfish for unit tests. Catches EngineTerminatedError and attempts one restart."
    },
    {
      "id": "US-003",
      "title": "Implement SRS spaced repetition manager",
      "description": "As a developer, I need an SRS card manager using the SM-2 algorithm for tracking chess mistakes.",
      "acceptanceCriteria": [
        "scripts/srs.py exists with SRSManager class and SM-2 algorithm",
        "CLI commands: due, review <card_id> <quality>, add (with --fen --player-move --best-move --cp-loss --classification flags), stats",
        "Cards stored in data/srs_cards.json",
        "Card schema: id(uuid), fen, player_move, best_move, cp_loss, classification, motif, explanation, created_at, next_review, interval_hours, ease_factor, repetitions, quality_history",
        "Timestamps stored as ISO 8601 strings",
        "SM-2 intervals in hours: 4, 24, 72, 168, 336, 720; after 720h multiply by ease_factor",
        "Failed cards (quality < 3) reset to interval_hours=4, repetitions=0",
        "Ease factor formula: EF' = EF + (0.1 - (5-q)*(0.08 + (5-q)*0.02)), minimum 1.3",
        "get_due_cards() returns cards where next_review <= now",
        "sm2_update() correctly updates interval, ease_factor, repetitions",
        "All CLI commands output JSON to stdout",
        "Corrupted JSON: backup file and start fresh with []",
        "python -m py_compile scripts/srs.py passes",
        "python -c \"from scripts.srs import SRSManager\" passes",
        "tests/test_srs.py exists with pytest tests for interval progression, failed reset, ease factor bounds, due filtering",
        "python -m pytest tests/test_srs.py passes"
      ],
      "priority": 3,
      "passes": true,
      "notes": "Quality scale 0-5. ValueError for quality out of range or card not found."
    },
    {
      "id": "US-004",
      "title": "MCP server - core game tools",
      "description": "As a developer, I need an MCP server with core game loop tools (new_game, get_board, make_move, engine_move).",
      "acceptanceCriteria": [
        "mcp-server/server.py exists using FastMCP from mcp.server.fastmcp",
        "mcp-server/requirements.txt lists: mcp[cli], python-chess",
        "Server name: chess-speedrun",
        "Games stored in memory dict keyed by game_id (UUID)",
        "Tool: new_game(target_elo, player_color, starting_fen) - starts game, returns GameState dict",
        "Tool: get_board(game_id) - returns current GameState dict with legal moves",
        "Tool: make_move(game_id, move) - player move in SAN notation, returns updated GameState",
        "Tool: engine_move(game_id) - engine makes move at configured difficulty, returns updated GameState",
        "After every make_move and engine_move, writes data/current_game.json using GameState from scripts.models",
        "Atomic file write: write to temp file then os.replace()",
        "Creates data/ directory if it doesn't exist on first write",
        "Invalid game_id returns error dict: {\"error\": \"Game not found: <id>\"}",
        "Illegal move returns error dict with legal moves list",
        "Move when game over returns error dict with result",
        "python -m py_compile mcp-server/server.py passes",
        "python -c \"import mcp.server.fastmcp\" passes"
      ],
      "priority": 4,
      "passes": true,
      "notes": "Imports ChessEngine from scripts.engine and GameState from scripts.models. sys.path manipulation may be needed for imports."
    },
    {
      "id": "US-005",
      "title": "MCP server - analysis tools",
      "description": "As a developer, I need analysis and evaluation tools added to the MCP server (analyze_position, evaluate_move, set_difficulty).",
      "acceptanceCriteria": [
        "Tool: analyze_position(fen, depth=20, multipv=3) added to mcp-server/server.py",
        "analyze_position works on any FEN (no active game required), creates temp ChessEngine",
        "Returns: {fen, depth, lines: [{rank, score_cp, moves, mate_in}]}",
        "Tool: evaluate_move(game_id, move) added - evaluates WITHOUT making the move",
        "Returns MoveEvaluation as dict (move_san, best_move_san, cp_loss, classification, etc.)",
        "Tool: set_difficulty(game_id, target_elo) added - changes difficulty mid-game",
        "set_difficulty clamps Elo to 100-3500 range",
        "Invalid FEN in analyze_position returns error dict",
        "evaluate_move on finished game returns error dict",
        "python -m py_compile mcp-server/server.py passes"
      ],
      "priority": 5,
      "passes": true,
      "notes": "Appends tools to existing mcp-server/server.py from US-004. Does not create new files."
    },
    {
      "id": "US-006",
      "title": "MCP server - utility tools",
      "description": "As a developer, I need utility tools added to the MCP server (PGN, legal moves, undo, set_position, SRS integration).",
      "acceptanceCriteria": [
        "Tool: get_game_pgn(game_id) added - returns valid PGN string",
        "Tool: get_legal_moves(game_id, square=None) added - lists legal moves, optionally filtered by source square",
        "Tool: undo_move(game_id) added - undoes last move; if last two were player+engine, undoes both",
        "undo_move updates data/current_game.json after undo",
        "undo_move on empty game (no moves) returns error dict",
        "Tool: set_position(fen) added - creates new game from custom FEN for puzzles, defaults to full-strength engine",
        "Tool: srs_add_card(game_id, move, explanation) added - saves current position as SRS mistake card via scripts.srs",
        "get_legal_moves on empty square returns empty list",
        "set_position with invalid FEN returns error dict",
        "PGN of game with no moves returns valid minimal PGN",
        "python -m py_compile mcp-server/server.py passes"
      ],
      "priority": 6,
      "passes": true,
      "notes": "Appends tools to existing mcp-server/server.py. srs_add_card is a bonus tool for tutor to save mistakes during games."
    },
    {
      "id": "US-007",
      "title": "Implement Terminal UI with Rich",
      "description": "As a user, I need a terminal chess board display that auto-updates during games by watching current_game.json.",
      "acceptanceCriteria": [
        "scripts/tui.py exists using Rich library",
        "Renders chess board with Unicode piece symbols (K=\u2654, Q=\u2655, R=\u2656, B=\u2657, N=\u2658, P=\u2659, etc.)",
        "Board has alternating dark/light square colors",
        "Highlights last move source/destination squares",
        "Board flips orientation if player is black",
        "Sidebar shows: move list (numbered SAN), eval bar with centipawn score, accuracy percentages",
        "Header shows: session number, streak, lesson name",
        "Layout: board on left (ratio 2), sidebar on right (ratio 1)",
        "Watches data/current_game.json using watchdog at ~4Hz (250ms interval)",
        "Graceful handling of partial/corrupt JSON writes (skip and wait for next poll)",
        "Missing current_game.json shows 'Waiting for game...' message",
        "data/sample_game.json created with a valid mid-game GameState",
        "python scripts/tui.py --sample renders sample board and exits (no watch loop)",
        "Reads GameState fields from JSON with fallback defaults for missing fields",
        "python -m py_compile scripts/tui.py passes",
        "python -c \"from scripts.tui import render_board\" passes"
      ],
      "priority": 7,
      "passes": true,
      "notes": "Standalone testing via --sample flag. No MCP dependency for testing. Imports GameState from scripts.models for type reference."
    },
    {
      "id": "US-008",
      "title": "Create curriculum and pedagogy reference files",
      "description": "As a chess tutor, I need reference materials for curriculum structure and teaching methodology.",
      "acceptanceCriteria": [
        "references/curriculum.md exists with full curriculum: Foundation (0-600), Tactical Basics (600-1000), Intermediate (1000-1500)",
        "Each phase has: lessons list, prerequisites, learning objectives",
        "references/chess-pedagogy.md exists with GM coaching methodology",
        "references/learning-science.md exists with deliberate practice and Zone of Proximal Development theory",
        "references/elo-milestones.md exists with specific skills per Elo range (at least 0-600, 600-1000, 1000-1500)",
        "Each file is well-structured with at least 3 major sections with headers",
        "Files cross-reference each other where relevant"
      ],
      "priority": 8,
      "passes": true,
      "notes": "Static content files. No code validation needed, just file existence and section structure."
    },
    {
      "id": "US-009",
      "title": "Create tactical patterns and mistakes reference files",
      "description": "As a chess tutor, I need reference materials for tactical patterns and common beginner mistakes.",
      "acceptanceCriteria": [
        "references/tactical-patterns.md exists covering: forks (knight/queen/pawn), pins (absolute/relative), skewers, discovered attacks, discovered check, double check, back-rank threats",
        "Each tactical pattern has: name, description, recognition cues, teaching approach",
        "references/common-mistakes.md exists covering: hanging pieces, ignoring threats, premature queen development, not castling, moving same piece twice, not controlling center",
        "Each mistake has: description, why beginners do it, teaching response",
        "references/opening-guide.md exists with at least 3 beginner-friendly openings with explanations",
        "Each file has well-structured sections with headers"
      ],
      "priority": 9,
      "passes": true,
      "notes": "Static content files. Must be created before SKILL.md (US-011) so it can reference them."
    },
    {
      "id": "US-010",
      "title": "Create curated puzzle sets with validation",
      "description": "As a chess tutor, I need puzzle positions organized by tactical motif with programmatic FEN validation.",
      "acceptanceCriteria": [
        "puzzles/forks.json exists with 10+ fork puzzle positions",
        "puzzles/pins.json exists with 10+ pin puzzle positions",
        "puzzles/skewers.json exists with 10+ skewer puzzle positions",
        "puzzles/back-rank.json exists with 10+ back-rank mate puzzles",
        "puzzles/checkmate-patterns.json exists with 10+ checkmate puzzles",
        "puzzles/beginner-endgames.json exists with 10+ endgame positions",
        "Each puzzle has: fen, solution_moves (UCI), solution_san, motif, difficulty, explanation",
        "scripts/validate_puzzles.py exists and validates all puzzle files",
        "Validation checks: FEN loads in python-chess without error, solution_moves are legal from position, sequential solution moves are each legal after previous",
        "python scripts/validate_puzzles.py exits with code 0",
        "python -m py_compile scripts/validate_puzzles.py passes"
      ],
      "priority": 10,
      "passes": true,
      "notes": "LLM-generated FENs are often invalid - programmatic validation is critical. Must be created before SKILL.md (US-011)."
    },
    {
      "id": "US-011",
      "title": "Create SKILL.md chess tutor skill file",
      "description": "As a Claude Code user, I need the SKILL.md file that transforms Claude Code into an adaptive chess tutor.",
      "acceptanceCriteria": [
        "SKILL.md exists with proper frontmatter (name, description, triggers: chess, play chess, chess lesson)",
        "Defines 3 expert roles: GM Teacher, Learning Psychologist, Behavioral Specialist",
        "Documents Quick Start flow: read progress, check MCP, load references, proceed",
        "Documents core loop: Play -> Analyze -> Teach -> Replay -> Plan",
        "Move evaluation thresholds: <=30 acknowledge, 31-80 mention, 81-200 teach, >200 intervene",
        "Language adaptation by Elo range (<600 simple, 600-1000 moderate, 1000-1500 technical)",
        "Post-game flow: save PGN, show summary, identify teaching positions, offer replay",
        "Session ending flow: save log, update progress, generate 3-perspective plan",
        "Difficulty control table: accuracy vs Elo offset mapping",
        "SRS system documentation with SM-2 intervals",
        "Curriculum phases summary referencing references/curriculum.md",
        "3-Perspective lesson plan framework (GM, Psychologist, Behaviorist)",
        "File references section pointing to actual files in references/, puzzles/, data/ directories"
      ],
      "priority": 11,
      "passes": true,
      "notes": "All referenced files (references/*, puzzles/*) now exist from US-008, US-009, US-010. Use relative paths."
    },
    {
      "id": "US-012",
      "title": "Create export script",
      "description": "As a developer, I need an export script for progress reports and game summaries in markdown format.",
      "acceptanceCriteria": [
        "scripts/export.py exists with export_progress() and export_games() functions",
        "CLI: python scripts/export.py progress - exports progress report from data/progress.json",
        "CLI: python scripts/export.py games - exports game summaries from data/sessions/ and data/games/",
        "Output format: markdown to stdout",
        "Progress report includes: current level, sessions completed, streak, accuracy trends, areas for improvement, SRS status",
        "Games report includes: per-game date, opponent Elo, result, accuracy, key mistakes",
        "No progress file: prints 'No progress data found. Play some games first!'",
        "No game files: prints 'No games found.'",
        "Corrupted JSON files: skip with warning",
        "python -m py_compile scripts/export.py passes",
        "python -c \"from scripts.export import export_progress, export_games\" passes"
      ],
      "priority": 12,
      "passes": true,
      "notes": "Markdown format chosen for human readability and Claude Code consumption."
    },
    {
      "id": "US-013",
      "title": "Create Claude settings and run integration verification",
      "description": "As a developer, I need .claude/settings.json for MCP server config and final integration verification.",
      "acceptanceCriteria": [
        ".claude/settings.json exists with mcpServers config",
        "MCP config: command=python, args=[mcp-server/server.py]",
        "Creates .claude/ directory if it doesn't exist",
        "Merges with existing settings.json if present (don't overwrite)",
        "python -m py_compile passes on ALL .py files in scripts/ and mcp-server/",
        "python scripts/validate_puzzles.py exits with code 0",
        "All required directories exist: data/sessions, data/games, data/lesson_plans, references/, puzzles/, mcp-server/, scripts/, .claude/",
        "All required files exist: scripts/install.sh, scripts/engine.py, scripts/srs.py, scripts/tui.py, scripts/export.py, scripts/models.py, scripts/validate_puzzles.py, mcp-server/server.py, SKILL.md"
      ],
      "priority": 13,
      "passes": true,
      "notes": "Final story - verification that all pieces are in place. Should be the last story implemented."
    },
    {
      "id": "US-014",
      "title": "Auto-save PGN on game over",
      "description": "As a user, I need completed games to be automatically saved as PGN files so no game data is lost when the post-game flow is interrupted.",
      "acceptanceCriteria": [
        "Add `from datetime import datetime, timezone` import to mcp-server/server.py",
        "New helper function `_auto_save_pgn(game_id, game)` in mcp-server/server.py near `_sync_game_json`",
        "Builds PGN from board.move_stack using chess.pgn.Game",
        "PGN headers include: Event='Chess Speedrun', Site='Chess Rocket', Date (UTC), Result",
        "PGN White/Black headers include Elo labels: 'Player (Elo N)' vs 'Stockfish (Elo N)'",
        "Player Elo read from data/progress.json (fallback to 'unknown' if file missing or unreadable)",
        "Writes atomically to data/games/game_{timestamp}_{game_id[:8]}.pgn (temp file + os.replace)",
        "Creates data/games/ directory if it doesn't exist",
        "Called automatically in make_move() after board.push() when board.is_game_over() is True",
        "Called automatically in engine_move() after board.push() when board.is_game_over() is True",
        "Does NOT affect the return value of make_move or engine_move (fire-and-forget save)",
        "python -m py_compile mcp-server/server.py passes",
        "python -c \"import mcp.server.fastmcp\" passes"
      ],
      "priority": 14,
      "passes": true,
      "notes": "See plan.md section 'US-014' for implementation details. Modifies existing make_move and engine_move functions in mcp-server/server.py."
    },
    {
      "id": "US-015",
      "title": "Add save_session MCP tool",
      "description": "As a tutor, I need a single MCP tool call that persists all session data (progress, session log) so nothing is lost between games.",
      "acceptanceCriteria": [
        "New @mcp.tool() save_session(game_id, estimated_elo=None, accuracy_pct=None, lesson_name='', areas_for_improvement=None, summary='') in mcp-server/server.py",
        "Loads data/progress.json with fallback to defaults if missing: {current_elo: 400, estimated_elo: 400, sessions_completed: 0, streak: 0, total_games: 0, accuracy_history: [], areas_for_improvement: [], last_session: null}",
        "Updates progress: current_elo and estimated_elo set to estimated_elo param (or keep existing if None)",
        "Increments sessions_completed by 1",
        "Increments total_games by 1",
        "Increments streak by 1 (always, regardless of game result)",
        "Appends accuracy_pct to accuracy_history (if not None)",
        "Sets last_session to current ISO 8601 UTC timestamp",
        "Sets areas_for_improvement if provided (replaces existing list)",
        "Writes data/progress.json atomically (temp file + os.replace)",
        "Writes session log to data/sessions/session_NNN.json (NNN = zero-padded sessions_completed)",
        "Session log contains: session_id, game_id, date (ISO 8601), result, player_color, target_elo, estimated_elo, total_moves, accuracy_pct, lesson_name, areas_for_improvement, summary",
        "Creates data/sessions/ directory if it doesn't exist",
        "Returns dict with 'message', 'session_id', 'session_file', 'progress' keys",
        "Invalid game_id returns error dict: {\"error\": \"Game not found: <id>\"}",
        "python -m py_compile mcp-server/server.py passes"
      ],
      "priority": 15,
      "passes": true,
      "notes": "See plan.md section 'US-015' for implementation details. Appends new tool to existing mcp-server/server.py."
    },
    {
      "id": "US-016",
      "title": "Add create_srs_cards_from_game MCP tool",
      "description": "As a tutor, I need a tool that batch-analyzes a completed game and creates SRS cards for all significant mistakes in one call.",
      "acceptanceCriteria": [
        "New @mcp.tool() create_srs_cards_from_game(game_id, cp_threshold=80) in mcp-server/server.py",
        "Returns error dict if game is not over (board.is_game_over() is False)",
        "Returns error dict if game_id not found",
        "Creates fresh ChessEngine at full strength (set_difficulty(3000)) for analysis",
        "Replays game from game['starting_fen'], evaluates each player move at depth 20",
        "Determines player moves based on game['player_color'] and board turn",
        "For moves with cp_loss >= cp_threshold: creates SRS card via SRSManager.add_card()",
        "SRS card explanation includes move number, player move, best move, and cp_loss",
        "Closes analysis engine after completion (analysis_engine.close())",
        "Returns dict with: game_id, total_player_moves, mistakes_found, cards_created, mistakes list (each with fen, move_number, player_move, best_move, cp_loss, classification), card_ids",
        "Game with no moves returns: {game_id, total_player_moves: 0, mistakes_found: 0, cards_created: 0, mistakes: [], card_ids: []}",
        "python -m py_compile mcp-server/server.py passes"
      ],
      "priority": 16,
      "passes": true,
      "notes": "See plan.md section 'US-016' for implementation details. Uses fixed depth 20 analysis (15-30s for full game is acceptable). Appends new tool to existing mcp-server/server.py."
    },
    {
      "id": "US-017",
      "title": "Fix export.py estimated_elo key mismatch",
      "description": "As a developer, I need export.py to correctly read the player's Elo from progress.json regardless of which key name is used.",
      "acceptanceCriteria": [
        "scripts/export.py line 39 updated: progress.get('estimated_elo', progress.get('current_elo', 'Unknown'))",
        "export_progress() works with progress.json containing only 'current_elo' key",
        "export_progress() works with progress.json containing only 'estimated_elo' key",
        "export_progress() works with progress.json containing both keys (estimated_elo takes priority)",
        "python -m py_compile scripts/export.py passes",
        "python -c \"from scripts.export import export_progress, export_games\" passes"
      ],
      "priority": 17,
      "passes": true,
      "notes": "See plan.md section 'US-017'. One-line fix. The save_session tool (US-015) writes both keys for forward compatibility."
    },
    {
      "id": "US-018",
      "title": "Update CLAUDE.md and SKILL.md for post-game automation",
      "description": "As a tutor, I need CLAUDE.md and SKILL.md to reference the new automated tools instead of manual file write instructions.",
      "acceptanceCriteria": [
        "CLAUDE.md Post-Game Flow section updated: notes PGN is auto-saved on game over",
        "CLAUDE.md Post-Game Flow section updated: references create_srs_cards_from_game(game_id) for batch SRS",
        "CLAUDE.md Session End Flow section updated: references save_session(game_id, ...) instead of manual file writes",
        "CLAUDE.md MCP Tools Reference table updated to include save_session and create_srs_cards_from_game",
        "SKILL.md section 7 (Post-Game Flow) updated to reference auto-saved PGN and create_srs_cards_from_game",
        "SKILL.md section 8 (Session Ending Flow) updated to reference save_session tool",
        "SKILL.md section 13 (File References) system files table updated to mention new tools",
        "Both files are valid markdown with no broken formatting"
      ],
      "priority": 18,
      "passes": true,
      "notes": "See plan.md section 'US-018'. Documentation-only changes. No code validation needed."
    },
    {
      "id": "US-019",
      "title": "End-to-end post-game flow integration test",
      "description": "As a developer, I need a pytest integration test that exercises the full post-game chain (auto PGN, save_session, batch SRS, export) using real Stockfish.",
      "acceptanceCriteria": [
        "tests/test_post_game.py exists",
        "Test imports server functions directly from mcp-server/server.py (not via MCP protocol)",
        "Test calls new_game() to start a game",
        "Test plays a short game to completion (e.g., use set_position with a near-mate FEN, then make the mating move)",
        "Test verifies PGN was auto-saved: at least one .pgn file exists in data/games/ after game over",
        "Test calls save_session() and verifies data/progress.json was updated (sessions_completed > 0, streak > 0)",
        "Test verifies a session log file was created in data/sessions/",
        "Test calls create_srs_cards_from_game() and verifies it returns without error (cards_created >= 0)",
        "Test calls export_progress() from scripts.export and verifies it returns non-empty markdown string",
        "Test cleans up test data files after run (restore original progress.json and srs_cards.json, remove test PGN and session files)",
        "Uses real Stockfish (requires Stockfish installed)",
        "python -m py_compile tests/test_post_game.py passes",
        "uv run python -m pytest tests/test_post_game.py -v passes",
        "uv run python -m pytest tests/ passes (all existing tests still pass)"
      ],
      "priority": 19,
      "passes": true,
      "notes": "See plan.md section 'US-019'. Final verification story. Uses real Stockfish for true integration testing. Test should use pytest fixtures for setup/teardown of data files."
    },
    {
      "id": "US-020",
      "title": "Build openings data pipeline",
      "description": "As a developer, I need a build script that downloads 3,627 chess openings from Lichess and creates a SQLite database + JSON trie for fast lookup.",
      "acceptanceCriteria": [
        "scripts/build_openings_db.py exists and is runnable via uv run python scripts/build_openings_db.py",
        "Downloads 5 TSV files from https://github.com/lichess-org/chess-openings/raw/master/{a,b,c,d,e}.tsv",
        "Caches downloaded TSVs in data/openings_raw/ to avoid re-downloading on subsequent runs",
        "Creates data/openings.db (SQLite) with table 'openings' containing columns: id, eco_volume, eco, name, family, pgn, uci, epd, num_moves",
        "eco_volume derived from first character of eco code (A-E)",
        "family derived from opening name before ':' (or full name if no ':')",
        "num_moves is count of space-split UCI moves",
        "SQLite indexes on: eco, eco_volume, family, name",
        "Creates data/openings_trie.json - nested dict keyed on UCI moves with '_eco' and '_name' at named nodes",
        "data/openings.db contains exactly 3,627 rows (or close, matching Lichess source)",
        "data/openings_trie.json is valid JSON and under 1MB",
        "Delete the chess-openings/ directory (LFS Parquet replaced by TSV download)",
        "Add data/openings_raw/, data/openings.db, data/openings_trie.json to .gitignore",
        "Add 'uv run python scripts/build_openings_db.py' step to scripts/install.sh",
        "Zero new dependencies - uses only stdlib (urllib.request, csv, sqlite3, json)",
        "Clear error message if download fails (no internet): prints URL that failed and exits with code 1",
        "python -m py_compile scripts/build_openings_db.py passes",
        "uv run python scripts/build_openings_db.py completes without error"
      ],
      "priority": 20,
      "passes": true,
      "notes": "See ~/.claude/plans/crispy-weaving-candy.md Phase 1. Source: Lichess GitHub TSVs (~200KB total). The chess-openings/ directory with LFS Parquet pointer should be deleted."
    },
    {
      "id": "US-021",
      "title": "Opening recognition library",
      "description": "As a developer, I need an OpeningsDB class that provides fast opening identification via trie lookup and rich querying via SQLite.",
      "acceptanceCriteria": [
        "scripts/openings.py exists with OpeningsDB class",
        "Constructor takes db_path and trie_path parameters with defaults pointing to data/openings.db and data/openings_trie.json",
        "Trie loaded into memory once at construction time",
        "identify_opening(uci_moves: list[str]) returns dict with eco, eco_volume, name, family, pgn, moves_matched keys, or None if no match",
        "identify_opening returns the DEEPEST matching named node (most specific opening)",
        "identify_opening returns None when current moves don't match any trie path (out of book)",
        "get_continuations(uci_moves: list[str]) returns list of dicts for named openings branching from current position",
        "search_openings(query, eco=None, eco_volume=None, limit=20) does LIKE search on name and eco columns",
        "get_opening_by_eco(eco) returns all openings matching an ECO code",
        "get_opening_lines(family) returns all variations of an opening family",
        "get_openings_for_level(elo) returns level-appropriate openings: Phase 1 (0-600) <= 4 half-moves common families, Phase 2 (600-1000) 4-10 half-moves, Phase 3 (1000+) full access",
        "get_random_opening(eco_volume=None, max_moves=None) returns a random opening with optional filters",
        "Graceful degradation: if trie/db files don't exist, methods return None or empty lists (no crash, no exception)",
        "All methods return plain dicts (not dataclasses), matching MCP server pattern",
        "SQLite connections opened per-query for thread safety",
        "python -m py_compile scripts/openings.py passes",
        "python -c \"from scripts.openings import OpeningsDB\" passes"
      ],
      "priority": 21,
      "passes": true,
      "notes": "See ~/.claude/plans/crispy-weaving-candy.md Phase 2. Depends on US-020 (needs data/openings.db and data/openings_trie.json). Memory ~4MB for trie."
    },
    {
      "id": "US-022",
      "title": "Opening MCP tools",
      "description": "As a tutor, I need MCP tools to identify, search, explore, suggest, and quiz openings so I can teach from the full 3,627-opening database.",
      "acceptanceCriteria": [
        "mcp-server/openings_tools.py exists with register_openings_tools(mcp, games, data_dir, project_root) function",
        "register_openings_tools registers 5 new @mcp.tool() decorated functions on the mcp instance",
        "Tool: identify_opening(game_id) - extracts UCI moves from game's board.move_stack, returns {eco, name, family, moves_matched, pgn} or None if out of book",
        "Tool: search_openings(query, eco=None, eco_volume=None, limit=20) - searches by name/ECO, returns {results: [...], total: int}",
        "Tool: get_opening_details(eco) - returns {eco, family, variations: [...]} for all openings with that ECO code",
        "Tool: suggest_opening(elo=None, color='white') - returns {suggestions: [{name, eco, pgn, epd}...]} appropriate for level. No 'why' field - Claude explains using references. Reads elo from data/progress.json if param is None",
        "Tool: opening_quiz(eco=None, difficulty='beginner') - picks opening, plays N-1 moves, calls set_position() to create real game, returns {game_id, opening_name, opening_eco, position_fen, moves_so_far, correct_move_san}",
        "opening_quiz tracks quizzed openings in data/progress.json under 'openings_studied' list to avoid repetition",
        "opening_quiz resets openings_studied when all openings at that level have been covered",
        "All 5 tools return error dicts {\"error\": \"Openings database not built. Run: uv run python scripts/build_openings_db.py\"} if openings.db or trie.json don't exist",
        "Invalid game_id in identify_opening returns error dict",
        "mcp-server/server.py modified to import and call register_openings_tools() before mcp.run()",
        "python -m py_compile mcp-server/openings_tools.py passes",
        "python -m py_compile mcp-server/server.py passes"
      ],
      "priority": 22,
      "passes": true,
      "notes": "See ~/.claude/plans/crispy-weaving-candy.md Phase 3. Depends on US-021. server.py is 864 lines - new tools MUST go in openings_tools.py, not server.py. Only ~15 lines added to server.py for import and registration."
    },
    {
      "id": "US-023",
      "title": "Live game opening integration",
      "description": "As a user, I need the tutor and TUI to show what opening I'm playing during a game, clearing when I leave book.",
      "acceptanceCriteria": [
        "mcp-server/server.py _build_game_state() calls openings_db.identify_opening() with UCI moves from game's board.move_stack",
        "GameState dict includes 'current_opening' field: dict with {eco, name, family, moves_matched} or None",
        "current_opening is set to None when game moves no longer match any trie path (out of book)",
        "current_opening updates after every make_move and engine_move call",
        "scripts/models.py GameState dataclass has new field: current_opening: dict | None = None",
        "scripts/tui.py displays opening name + ECO code in sidebar when current_opening is not None",
        "TUI display format: 'Opening: {name} ({eco})' or similar",
        "When current_opening is None (out of book), TUI shows nothing or 'Out of book' for opening",
        "data/current_game.json includes current_opening field after moves",
        "Opening identification adds no perceptible latency (trie lookup is O(d) where d = moves played)",
        "If openings DB not built, current_opening is simply None (graceful degradation, no crash)",
        "python -m py_compile mcp-server/server.py passes",
        "python -m py_compile scripts/models.py passes",
        "python -m py_compile scripts/tui.py passes",
        "uv run python scripts/tui.py --sample still works"
      ],
      "priority": 23,
      "passes": true,
      "notes": "See ~/.claude/plans/crispy-weaving-candy.md Phase 4. Depends on US-021 and US-022. Key behavior: CLEAR current_opening to None when out of book (don't keep showing last known opening)."
    },
    {
      "id": "US-024",
      "title": "Opening puzzle generation",
      "description": "As a tutor, I need opening-based puzzles in two styles: 'next book move' knowledge tests and 'opening trap' refutation puzzles.",
      "acceptanceCriteria": [
        "scripts/generate_opening_puzzles.py exists",
        "CLI: uv run python scripts/generate_opening_puzzles.py creates both puzzle files",
        "puzzles/opening-moves.json created with ~30 'next book move' puzzles",
        "opening-moves puzzles: FEN is position after N-1 moves, solution is move N from the opening PGN",
        "opening-moves motif field is 'opening'",
        "opening-moves difficulty: 1-4 half-moves = beginner, 5-8 = intermediate, 9+ = advanced",
        "opening-moves covers all 5 ECO volumes (A through E)",
        "puzzles/opening-traps.json created with ~20 opening trap puzzles",
        "opening-traps includes ~10 curated famous traps (Scholar's Mate, Fishing Pole, Lasker Trap, Legal's Mate, Englund Gambit Trap, etc.) with hardcoded FENs and refutations",
        "opening-traps includes ~10 auto-generated traps: positions where a natural bad move leads to >150cp punishment, verified by Stockfish",
        "opening-traps motif field is 'opening_trap'",
        "Both files use existing puzzle schema: {fen, solution_moves (UCI), solution_san, motif, difficulty, explanation}",
        "All generated FENs are valid (loadable by python-chess)",
        "All solution_moves are legal from the given FEN position",
        "scripts/validate_puzzles.py updated to include 'opening-moves.json' and 'opening-traps.json' in expected files list",
        "uv run python scripts/validate_puzzles.py exits with code 0 (all puzzle files valid)",
        "python -m py_compile scripts/generate_opening_puzzles.py passes"
      ],
      "priority": 24,
      "passes": true,
      "notes": "See ~/.claude/plans/crispy-weaving-candy.md Phase 5. Depends on US-020 (reads from data/openings.db). Trap generation uses Stockfish - run once, commit the JSON output. Curated traps are hardcoded in the script."
    },
    {
      "id": "US-025",
      "title": "Opening integration documentation updates",
      "description": "As a tutor and developer, I need CLAUDE.md, SKILL.md, and reference files updated to document the new opening tools, puzzles, and teaching workflow.",
      "acceptanceCriteria": [
        "CLAUDE.md MCP Tools Reference table includes all 5 new tools: identify_opening, search_openings, get_opening_details, suggest_opening, opening_quiz",
        "CLAUDE.md Data Files table includes data/openings.db and data/openings_trie.json",
        "CLAUDE.md Core Loop Play phase mentions opening identification after each move",
        "CLAUDE.md has new 'Opening Study Mode' section describing how to use search/suggest/quiz tools",
        "CLAUDE.md puzzle table updated to include puzzles/opening-moves.json and puzzles/opening-traps.json",
        "SKILL.md mirrors all CLAUDE.md changes (new tools, opening study mode, puzzles)",
        "references/opening-guide.md updated with note about full 3,627-opening database accessible via search_openings and get_opening_details tools",
        "references/curriculum.md updated with opening study goals per phase: Phase 1 learn 2-3 basic openings, Phase 2 expand to 5-6 with ECO families, Phase 3 full repertoire with opening traps",
        "scripts/install.sh includes 'uv run python scripts/build_openings_db.py' step",
        "All modified files are valid markdown with no broken formatting",
        "All modified .sh files are executable"
      ],
      "priority": 25,
      "passes": true,
      "notes": "See ~/.claude/plans/crispy-weaving-candy.md Phase 6. Depends on US-022, US-023, US-024 (documents tools and features from those stories). Documentation-only plus install.sh update."
    },
    {
      "id": "US-026",
      "title": "Opening integration tests",
      "description": "As a developer, I need comprehensive tests for the openings library, MCP tools, and live game integration.",
      "acceptanceCriteria": [
        "tests/test_openings.py exists with pytest tests",
        "Trie tests: identify_opening(['e2e4', 'c7c5']) returns Sicilian Defense (B20)",
        "Trie tests: identify_opening([]) returns None",
        "Trie tests: identify_opening with non-book moves returns None (out of book)",
        "Trie tests: identify_opening returns deepest match (most specific variation)",
        "SQLite tests: search_openings('Italian') returns Italian Game variations",
        "SQLite tests: search by ECO code returns correct results",
        "SQLite tests: get_openings_for_level(400) returns only short/common openings",
        "SQLite tests: get_openings_for_level(1200) returns wider selection",
        "Graceful degradation test: OpeningsDB with nonexistent paths returns None/empty (no crash)",
        "MCP tool tests: identify_opening with valid game_id returns opening info",
        "MCP tool tests: search_openings returns results with total count",
        "MCP tool tests: suggest_opening returns level-appropriate list",
        "MCP tool tests: opening_quiz creates a game and returns correct_move_san",
        "MCP tool tests: all tools return error dict when openings DB not built",
        "Quiz tracking test: opening_quiz updates openings_studied in progress.json",
        "python -m py_compile tests/test_openings.py passes",
        "uv run python -m pytest tests/test_openings.py -v passes",
        "uv run python -m pytest tests/ -v passes (no regressions on existing tests)"
      ],
      "priority": 26,
      "passes": true,
      "notes": "See ~/.claude/plans/crispy-weaving-candy.md Phase 7. Depends on US-021, US-022, US-023. Tests require data/openings.db and data/openings_trie.json to exist (run build_openings_db.py first). Use pytest fixtures for test data setup/teardown."
    },
    {
      "id": "US-027",
      "title": "Create response schemas and minification module",
      "description": "As a developer, I need a response_schemas module that minifies MCP tool responses to reduce LLM context token waste while keeping data/current_game.json (TUI sync) unchanged.",
      "acceptanceCriteria": [
        "mcp-server/response_schemas.py exists (~200 lines)",
        "Function minify_game_state(state: dict) -> dict exists, used by 6 tools (new_game, get_board, make_move, engine_move, undo_move, set_position)",
        "minify_game_state removes: board_display, session_number, streak, lesson_name",
        "minify_game_state compacts move_list from JSON array to PGN string (e.g., '1.e4 e5 2.Nf3 Nc6 3.Bb5 a6') — full history preserved, ~40% shorter",
        "minify_game_state replaces legal_moves list with legal_moves_count integer",
        "minify_game_state simplifies accuracy dict to single float (player's color only)",
        "minify_game_state simplifies current_opening to {name, eco} only (drops family, moves_matched)",
        "Function minify_move_evaluation(evaluation: dict) -> dict exists, used by evaluate_move",
        "minify_move_evaluation removes tactical_motif (always None) and is_best (redundant with cp_loss == 0)",
        "minify_move_evaluation truncates best_line to first 3 moves",
        "Function minify_analysis(analysis: dict) -> dict exists, used by analyze_position",
        "minify_analysis truncates PV moves to 5 moves max per line",
        "minify_analysis removes mate_in key when value is null/None",
        "Function minify_save_session(response: dict) -> dict exists, used by save_session",
        "minify_save_session simplifies progress sub-dict to {current_elo, sessions_completed, streak, total_games} only",
        "Dict-based validation schema GAME_STATE_SCHEMA exists — validates minified GameState keys and types",
        "Dict-based validation schema MOVE_EVALUATION_SCHEMA exists — validates minified MoveEvaluation",
        "Dict-based validation schema ANALYSIS_SCHEMA exists — validates analysis response",
        "Dict-based validation schema ERROR_SCHEMA exists — validates error responses {\"error\": str}",
        "Function validate_response(response, schema) -> list[str] exists — returns list of validation errors (empty = valid)",
        "Validation is enabled via CHESS_SPEEDRUN_VALIDATE=1 env var (always on in tests, off by default)",
        "python -m py_compile mcp-server/response_schemas.py passes",
        "python -c \"from response_schemas import minify_game_state, minify_move_evaluation, minify_analysis, minify_save_session, validate_response\" passes (from mcp-server/ directory)"
      ],
      "priority": 27,
      "passes": true,
      "notes": "Key constraint: only MCP tool return values get minified. data/current_game.json (TUI file sync) must keep ALL fields unchanged. PGN string format for move_list uses standard chess notation (1.e4 e5 2.Nf3 ...) which is natural for the LLM agent to read. See ~/.claude/plans/witty-moseying-seal.md Phase 1."
    },
    {
      "id": "US-028",
      "title": "Modify MCP server to return minified responses",
      "description": "As a developer, I need the MCP server tool functions to return minified responses via response_schemas, while keeping _sync_game_json() calls unchanged for TUI compatibility.",
      "acceptanceCriteria": [
        "mcp-server/server.py imports minify_game_state, minify_move_evaluation, minify_analysis, minify_save_session from response_schemas",
        "new_game() returns minify_game_state(state) instead of raw state",
        "get_board() returns minify_game_state(_build_game_state(...)) instead of raw state",
        "make_move() returns minify_game_state(state) instead of raw state",
        "engine_move() returns minify_game_state(state) instead of raw state",
        "undo_move() returns minify_game_state(state) instead of raw state",
        "set_position() returns minify_game_state(state) instead of raw state",
        "analyze_position() returns minify_analysis({...}) instead of raw dict",
        "evaluate_move() returns minify_move_evaluation(asdict(evaluation)) instead of raw asdict",
        "save_session() returns minify_save_session({...}) instead of raw dict",
        "All _sync_game_json(state) calls remain unchanged — full state written to data/current_game.json for TUI",
        "Error dict returns (e.g., {\"error\": \"Game not found\"}) are NOT wrapped by minification — passed through as-is",
        "~30 lines changed total in server.py",
        "python -m py_compile mcp-server/server.py passes"
      ],
      "priority": 28,
      "passes": true,
      "notes": "Depends on US-027. The _sync_game_json() calls happen BEFORE the return statements, so they still get the full unminified state dict. Only the return values to the MCP client (Claude) are minified. See ~/.claude/plans/witty-moseying-seal.md Phase 2."
    },
    {
      "id": "US-029",
      "title": "Trim openings_tools.py response sizes",
      "description": "As a developer, I need minor trims to opening tool responses to remove unnecessary fields from MCP responses.",
      "acceptanceCriteria": [
        "suggest_opening() in mcp-server/openings_tools.py no longer includes epd field in suggestion dicts",
        "Suggestion dicts contain only: name, eco, pgn (epd removed)",
        "get_opening_details() remains unchanged (variations are the whole point of the tool)",
        "All other opening tools (identify_opening, search_openings, opening_quiz) remain unchanged",
        "~5 lines changed in openings_tools.py",
        "python -m py_compile mcp-server/openings_tools.py passes"
      ],
      "priority": 29,
      "passes": true,
      "notes": "Small independent change. epd field is not used by the tutor agent — it rebuilds positions from PGN when needed. See ~/.claude/plans/witty-moseying-seal.md Phase 3."
    },
    {
      "id": "US-030",
      "title": "Create shared test fixtures with dual-mode support",
      "description": "As a developer, I need a tests/conftest.py with shared fixtures supporting both mocked (fast, no Stockfish) and real (--e2e) test modes.",
      "acceptanceCriteria": [
        "tests/conftest.py exists (~100 lines)",
        "pytest_addoption() registers --e2e CLI flag for real Stockfish tests",
        "Fixture mock_chess_engine patches ChessEngine class with a mock that returns valid chess moves — skipped when --e2e flag is passed",
        "Mock engine's get_engine_move returns a valid legal move from the board",
        "Mock engine's evaluate_move returns a MoveEvaluation with realistic fields",
        "Mock engine's analyze_position returns realistic analysis lines",
        "Mock engine's set_difficulty is a no-op",
        "Mock engine's close is a no-op",
        "Fixture clean_data_dir backs up and restores data/progress.json, data/srs_cards.json, and cleans up test-created files in data/games/ and data/sessions/ after each test",
        "Fixture enable_validation sets CHESS_SPEEDRUN_VALIDATE=1 env var for the test session and restores original value after",
        "pytest marker @pytest.mark.e2e registered — tests with this marker only run when --e2e is passed",
        "uv run pytest tests/ runs fast with mocked engine (no Stockfish required)",
        "uv run pytest tests/ --e2e runs with real Stockfish for full end-to-end testing",
        "Existing tests in test_engine.py, test_srs.py, test_openings.py, test_post_game.py are not broken by conftest.py",
        "python -m py_compile tests/conftest.py passes"
      ],
      "priority": 30,
      "passes": true,
      "notes": "Depends on US-027 (for enable_validation fixture using CHESS_SPEEDRUN_VALIDATE). The mock_chess_engine must return legal moves — use python-chess board.legal_moves to pick a valid move. Existing test files (test_post_game.py, test_openings.py) use real Stockfish and have their own fixtures — conftest should not break them. See ~/.claude/plans/witty-moseying-seal.md Phase 4."
    },
    {
      "id": "US-031",
      "title": "Create per-tool MCP integration tests",
      "description": "As a developer, I need comprehensive per-tool integration tests verifying minified response shapes, TUI JSON integrity, and schema validation for all 14 MCP server tools.",
      "acceptanceCriteria": [
        "tests/test_mcp_tools.py exists (~550 lines)",
        "Imports server functions via importlib from mcp-server/server.py (same pattern as test_post_game.py)",
        "Uses mock_chess_engine fixture from conftest.py (runs without Stockfish)",
        "Uses enable_validation fixture (schema validation active)",
        "TestNewGame class: verifies minified response has no board_display, has legal_moves_count (int not list), move_list is PGN string (not array), no session_number/streak/lesson_name; verifies data/current_game.json has full state with ALL original fields including board_display and legal_moves list; asserts len(json.dumps(response)) < 800 chars",
        "TestGetBoard class: verifies minified response shape, verifies get_board does NOT write to current_game.json",
        "TestMakeMove class: verifies minified response, move_list PGN string grows after move, syncs full state to JSON file",
        "TestEngineMove class: verifies minified response, syncs full state to JSON file",
        "TestAnalyzePosition class: verifies PV moves truncated to 5 per line, null mate_in keys removed, invalid FEN returns error dict",
        "TestEvaluateMove class: verifies tactical_motif stripped, is_best stripped, best_line truncated to 3 moves",
        "TestSetDifficulty class: verifies confirmation dict shape {game_id, target_elo, message}, Elo clamping to 100-3500",
        "TestGetGamePgn class: verifies PGN string returned in {pgn: str} dict",
        "TestGetLegalMoves class: verifies full move list returned (NOT minified — this is the dedicated tool for getting legal moves)",
        "TestUndoMove class: verifies minified state returned, undo-pair logic (undoes both player+engine moves)",
        "TestSetPosition class: verifies minified state returned, invalid FEN returns error dict",
        "TestSrsAddCard class: verifies card dict returned with expected keys",
        "TestSaveSession class: verifies minified progress in response (only current_elo, sessions_completed, streak, total_games), session file created in data/sessions/",
        "TestCreateSrsCards class: verifies batch result shape {game_id, total_player_moves, mistakes_found, cards_created, mistakes, card_ids}",
        "TestResponseSchemas class: validates every tool response against its schema via validate_response(), covers all error paths returning {\"error\": str}",
        "python -m py_compile tests/test_mcp_tools.py passes",
        "uv run python -m pytest tests/test_mcp_tools.py -v passes"
      ],
      "priority": 31,
      "passes": true,
      "notes": "Depends on US-027, US-028, US-030. 14 test classes (one per server.py tool) plus TestResponseSchemas. Uses mocked engine so runs fast without Stockfish. Key insight: data/current_game.json must have FULL unminified state while MCP return values are minified. See ~/.claude/plans/witty-moseying-seal.md Phase 5."
    },
    {
      "id": "US-032",
      "title": "Create multi-tool functional flow tests",
      "description": "As a developer, I need flow tests that exercise multi-tool sequences verifying minification across game lifecycle, opening study, SRS review, session persistence, error paths, and response size regression.",
      "acceptanceCriteria": [
        "tests/test_mcp_flows.py exists (~350 lines)",
        "Imports server functions via importlib from mcp-server/server.py",
        "Uses mock_chess_engine and enable_validation fixtures from conftest.py",
        "TestGameLifecycle class: new_game -> make_move -> engine_move loop -> game_over; all responses are minified (no board_display, legal_moves_count not list), TUI JSON (current_game.json) has full unminified state throughout",
        "TestOpeningStudyFlow class: suggest_opening -> opening_quiz -> make_move; verifies suggest_opening has no epd field, quiz creates playable game",
        "TestSrsReviewFlow class: play game to completion -> create_srs_cards_from_game -> verify cards created with expected shape",
        "TestSessionPersistence class: save_session -> verify progress.json updated + session file created; multiple save_session calls increment sessions_completed",
        "TestErrorPaths class: all tools that take game_id return {\"error\": str} for nonexistent game_id; covers new_game invalid FEN, make_move illegal move, analyze_position invalid FEN, set_position invalid FEN",
        "TestResponseSizeRegression class: after 20 moves, asserts len(json.dumps(game_state_response)) < 500 chars; analysis response < 400 chars; evaluation response < 200 chars — prevents response size regressions",
        "python -m py_compile tests/test_mcp_flows.py passes",
        "uv run python -m pytest tests/test_mcp_flows.py -v passes"
      ],
      "priority": 32,
      "passes": true,
      "notes": "Depends on US-027, US-028, US-029, US-030. Flow tests exercise realistic multi-tool sequences that the tutor agent would perform. Response size regression thresholds are the key deliverable — they prevent future changes from inflating response sizes. See ~/.claude/plans/witty-moseying-seal.md Phase 6."
    },
    {
      "id": "US-033",
      "title": "Update existing tests for minified response shapes",
      "description": "As a developer, I need existing test files updated so their assertions match the new minified response shapes from server.py tools.",
      "acceptanceCriteria": [
        "tests/test_post_game.py updated: save_session result['progress'] assertions updated for minified shape (only current_elo, sessions_completed, streak, total_games keys — no accuracy_history, areas_for_improvement, last_session)",
        "tests/test_post_game.py updated: any assertions checking for board_display, legal_moves list, session_number, streak, lesson_name in GameState responses are updated to check for minified equivalents (legal_moves_count, PGN move_list string)",
        "tests/test_post_game.py: _play_to_checkmate() helper updated if it checks final_state fields that are now minified",
        "tests/test_post_game.py: TestFullPostGameChain assertions updated for minified response shapes",
        "tests/test_openings.py updated: TestLiveGameOpeningIntegration assertions that check current_opening shape updated — minified current_opening only has {name, eco} (no family, moves_matched)",
        "tests/test_openings.py: test_current_opening_in_game_state_dict updated if it asserts presence of family or moves_matched in current_opening",
        "End-to-end with real Stockfish: uv run python -m pytest tests/ --e2e -v passes — all tests run with real Stockfish engine responses, no mocks",
        "Without --e2e flag: uv run python -m pytest tests/test_mcp_tools.py tests/test_mcp_flows.py -v passes — mocked engine tests run fast without Stockfish",
        "Existing e2e tests still pass: uv run python -m pytest tests/test_engine.py --e2e passes",
        "Existing e2e tests still pass: uv run python -m pytest tests/test_srs.py --e2e passes",
        "Existing e2e tests still pass: uv run python -m pytest tests/test_openings.py --e2e passes",
        "Existing e2e tests still pass: uv run python -m pytest tests/test_post_game.py --e2e passes",
        "Full suite with mocks: uv run python -m pytest tests/ -v passes with zero failures (no Stockfish required)",
        "Full suite with Stockfish: uv run python -m pytest tests/ --e2e -v passes with zero failures (real engine responses)"
      ],
      "priority": 33,
      "passes": true,
      "notes": "Depends on US-027, US-028. Must be done carefully — read each test file and identify every assertion that touches fields affected by minification. Key fields changed: board_display removed, legal_moves -> legal_moves_count, move_list array -> PGN string, accuracy dict -> float, current_opening simplified, save_session progress simplified. Acceptance requires BOTH modes pass: mocked (uv run pytest tests/) AND e2e (uv run pytest tests/ --e2e). See ~/.claude/plans/witty-moseying-seal.md Phase 7."
    },
    {
      "id": "US-034",
      "title": "Auto-track accuracy during gameplay",
      "description": "As a tutor, I need accuracy to be tracked automatically during games so it's never null in session data and the TUI shows real-time accuracy.",
      "acceptanceCriteria": [
        "New helper function _recompute_accuracy(game) in mcp-server/server.py",
        "_recompute_accuracy reads game['move_evals'], counts moves per color where cp_loss <= 30, updates game['accuracy'] dict",
        "evaluate_move() stores evaluation in game['move_evals'] list with fields: move_san, best_move_san, cp_loss, classification, color, ply",
        "evaluate_move() calls _recompute_accuracy(game) after storing eval",
        "evaluate_move() calls _sync_game_json() to push updated accuracy to TUI",
        "undo_move() filters game['move_evals'] to remove entries where ply >= len(board.move_stack) after undo",
        "undo_move() calls _recompute_accuracy(game) after filtering",
        "save_session() auto-computes accuracy_pct from game['move_evals'] when accuracy_pct param is None",
        "save_session() auto-compute: filters evals by player_color, calculates good_moves/total * 100, rounds to 1 decimal",
        "save_session() still accepts manual accuracy_pct override (backward compatible)",
        "When no move_evals exist and no accuracy_pct provided, accuracy_history is not appended to (stays unchanged)",
        "GameState.accuracy dict updates in real-time during gameplay (visible in data/current_game.json and TUI)",
        "tests/test_post_game.py has test_save_session_auto_computes_accuracy: verifies fallback computation from stored evals",
        "tests/test_post_game.py has test_save_session_no_evals_no_accuracy: verifies null stays null when no evals exist",
        "python -m py_compile mcp-server/server.py passes",
        "uv run python -m pytest tests/test_post_game.py -v passes",
        "uv run python -m pytest tests/ passes (no regressions)"
      ],
      "priority": 26,
      "passes": true,
      "notes": "Must run BEFORE US-027 (minification). Modifies evaluate_move(), save_session(), undo_move() in server.py — adds code BEFORE return statements, compatible with US-028 which wraps return values. See ~/.claude/plans/groovy-purring-flask.md for implementation details and root cause analysis."
    },
    {
      "id": "US-035",
      "title": "Create tactical motif detection module",
      "description": "As a developer, I need a motif_detector module that programmatically identifies tactical themes (fork, pin, skewer, back-rank mate, discovered attack, etc.) from a board position and a move, so generated puzzles can be auto-classified.",
      "acceptanceCriteria": [
        "scripts/motif_detector.py exists (~200 lines)",
        "detect_motif(board: chess.Board, move: chess.Move) -> str | None returns the primary motif",
        "detect_all_motifs(board: chess.Board, move: chess.Move) -> list[str] returns all motifs present",
        "Fork detection: after move, the moved piece attacks 2+ enemy pieces worth >= knight (3 points)",
        "Pin detection: after move, a sliding piece pins an enemy piece to their king or queen",
        "Skewer detection: after move, a sliding piece attacks a valuable piece with a less valuable piece behind it",
        "Back-rank mate detection: checkmate on 1st/8th rank where king is trapped by own pawns",
        "Discovered attack detection: moving piece unblocks a ray from another friendly piece attacking enemy",
        "Double check detection: board.checkers() count >= 2 after the move",
        "Promotion detection: move.promotion is not None",
        "Checkmate detection: board_after.is_checkmate()",
        "Returns None for quiet/positional moves with no tactical theme",
        "Helper _piece_value(piece_type) maps P=1, N=3, B=3, R=5, Q=9, K=100",
        "python -m py_compile scripts/motif_detector.py passes",
        "python -c \"from scripts.motif_detector import detect_motif, detect_all_motifs\" passes",
        "tests/test_motif_detector.py exists with pytest tests for each motif type using known positions",
        "Test: Nc7+ on r3k3/8/8/3N4/8/8/8/4K3 detected as fork",
        "Test: Re8# on 6k1/5ppp/8/8/8/8/8/4R1K1 detected as back-rank",
        "Test: Re1+ on 4q3/8/8/4k3/8/8/8/R6K detected as skewer",
        "Test: e2e4 from starting position detected as None (no motif)",
        "uv run python -m pytest tests/test_motif_detector.py -v passes"
      ],
      "priority": 35,
      "passes": true,
      "notes": "Foundation for all puzzle generation. No external dependencies beyond python-chess. See ~/.claude/plans/gentle-fluttering-curry.md Step 1."
    },
    {
      "id": "US-036",
      "title": "Create Stockfish self-play puzzle generator (Pipeline 1)",
      "description": "As a developer, I need a puzzle generation pipeline that creates realistic tactical puzzles by playing Stockfish against itself from known openings, then extracting positions with clear tactical solutions.",
      "acceptanceCriteria": [
        "scripts/generate_puzzles.py exists as the main puzzle generation orchestrator",
        "Function generate_stockfish_puzzles(engine, openings_db_path, target_per_motif=28, depth=18) -> dict[str, list[dict]]",
        "Self-play: picks random opening from data/openings.db (3-6 half-moves), plays 10-25 more moves at ~1500 Elo",
        "Analysis: at each position after move 8, runs full-strength analysis (depth 18, multipv 3)",
        "Puzzle candidate: best move is 150+ cp better than 2nd best move",
        "Runs motif detection via scripts.motif_detector.detect_motif() on each candidate",
        "Builds multi-move solutions: follows PV while opponent responses are forced (within 50cp)",
        "Solution lines capped at 6 half-moves (3 full moves)",
        "Difficulty rating: beginner (few pieces, 1-move, >300cp), intermediate (<=20 pieces, <=2 moves), advanced (complex)",
        "Numeric difficulty_rating estimated from piece count + eval swing + motif type (400-1800 range)",
        "Generates pedagogical explanations using templates per motif type",
        "Deduplication: normalizes FEN (strips move counters) to avoid duplicate positions",
        "Single Stockfish process reused across entire generation run",
        "Writes to 6 tactical puzzle files: forks.json, pins.json, skewers.json, back-rank.json, checkmate-patterns.json, beginner-endgames.json",
        "Each tactical file has 25+ puzzles after generation",
        "All generated puzzles pass scripts/validate_puzzles.py",
        "CLI: uv run python scripts/generate_puzzles.py --pipeline stockfish --target 28",
        "CLI supports --seed flag for reproducibility (default 42)",
        "python -m py_compile scripts/generate_puzzles.py passes"
      ],
      "priority": 36,
      "passes": true,
      "notes": "This is the slowest pipeline (~30-80 min) but produces the highest quality puzzles. Replaces the current artificial stripped-down positions with realistic game positions. Depends on US-035 (motif_detector). See ~/.claude/plans/gentle-fluttering-curry.md Pipeline 1."
    },
    {
      "id": "US-037",
      "title": "Create player game mining puzzle generator (Pipeline 2)",
      "description": "As a developer, I need a puzzle generation pipeline that extracts 'what should you have played?' puzzles from the player's own completed games, with incremental processing so new games are auto-mined.",
      "acceptanceCriteria": [
        "Function generate_game_puzzles(games_dir, depth=20, cp_threshold=100, already_processed=None) -> list[dict] in scripts/generate_puzzles.py",
        "Replays each PGN file in data/games/ move by move",
        "At each player move, runs full-strength analysis (depth 20)",
        "If cp_loss >= 100: creates puzzle with best move as solution",
        "Puzzle explanation includes: 'In your game, you played X (cp_loss: N). The best move was Y.'",
        "Runs motif detection to classify the tactic type",
        "Includes source metadata: source='game', source_file=PGN filename, move_number",
        "Writes to puzzles/from-games.json (new file)",
        "Deduplicates by FEN against existing puzzles in from-games.json",
        "puzzles/manifest.json created/updated with processed_files list",
        "Incremental: get_unprocessed_games(manifest, games_dir) returns only new PGN files",
        "CLI: uv run python scripts/generate_puzzles.py --pipeline games --incremental",
        "Processes existing 6 PGN files and generates puzzles from them",
        "All generated puzzles pass validation (legal FENs, legal solution moves)",
        "python -m py_compile scripts/generate_puzzles.py passes"
      ],
      "priority": 37,
      "passes": true,
      "notes": "Pattern follows mcp-server/server.py create_srs_cards_from_game() (line 852+) for game replay + analysis. Manifest enables incremental processing. Depends on US-035. See ~/.claude/plans/gentle-fluttering-curry.md Pipeline 2."
    },
    {
      "id": "US-038",
      "title": "Expand opening puzzle generation (Pipeline 3)",
      "description": "As a developer, I need the opening puzzle generator to produce more puzzles with better coverage, integrated into the unified generate_puzzles.py orchestrator.",
      "acceptanceCriteria": [
        "Function generate_opening_puzzles_expanded(openings_db_path, target_moves=40, target_traps=30) in scripts/generate_puzzles.py",
        "Delegates to existing scripts/generate_opening_puzzles.py logic but with higher targets",
        "opening-moves.json target increased from 35 to 40+ puzzles",
        "opening-traps.json target increased from 22 to 30+ puzzles",
        "Better ECO distribution: all 5 volumes (A-E) have at least 6 puzzles each in opening-moves",
        "Deeper Stockfish analysis for auto-trap detection (depth 18 vs current 15)",
        "Adds source='opening_db' field to generated puzzles",
        "CLI: uv run python scripts/generate_puzzles.py --pipeline openings",
        "All generated puzzles pass validation",
        "python -m py_compile scripts/generate_puzzles.py passes"
      ],
      "priority": 38,
      "passes": true,
      "notes": "Lower priority than Pipelines 1 and 2 since existing opening puzzles are already reasonable quality. Depends on US-035. See ~/.claude/plans/gentle-fluttering-curry.md Pipeline 3."
    },
    {
      "id": "US-039",
      "title": "Add generate_puzzles_from_game MCP tool",
      "description": "As a tutor, I need an MCP tool that automatically generates puzzles from a completed game and appends them to puzzles/from-games.json, so the puzzle library grows as the student plays more games.",
      "acceptanceCriteria": [
        "New @mcp.tool() generate_puzzles_from_game(game_id, cp_threshold=100) in mcp-server/server.py",
        "Returns error dict if game not found or game not over",
        "Replays game, evaluates each player move at full strength (depth 20)",
        "Creates puzzle for each position with cp_loss >= cp_threshold",
        "Runs motif detection on each puzzle",
        "Loads existing puzzles/from-games.json, appends new puzzles, deduplicates by FEN",
        "Atomic write: temp file + os.replace",
        "Returns dict with: game_id, puzzles_found, puzzles_added, total_game_puzzles, puzzle_file",
        "python -m py_compile mcp-server/server.py passes",
        "CLAUDE.md MCP Tools Reference updated to include generate_puzzles_from_game"
      ],
      "priority": 39,
      "passes": true,
      "notes": "Bridges the MCP server with Pipeline 2 for real-time puzzle generation during tutoring sessions. Depends on US-035, US-037. See ~/.claude/plans/gentle-fluttering-curry.md Step 6."
    },
    {
      "id": "US-040",
      "title": "Enhanced puzzle validation with Stockfish verification",
      "description": "As a developer, I need validate_puzzles.py upgraded to verify that puzzle solutions are actually the best moves (not just legal moves), and to detect incorrect motif classifications and false checkmate claims.",
      "acceptanceCriteria": [
        "scripts/validate_puzzles.py updated with new --engine-verify flag",
        "EXPECTED_FILES list updated to include 'from-games.json'",
        "When --engine-verify is passed: runs Stockfish at depth 15 on each puzzle position",
        "Engine verification checks: solution move is engine's #1 choice (or within 20cp of best)",
        "Engine verification checks: checkmate puzzles result in board.is_checkmate() after solution",
        "Engine verification checks: no stalemate in solution line (board.is_stalemate() is False at each step)",
        "Engine verification checks: motif classification matches detect_motif() result (warning, not error)",
        "Without --engine-verify: existing legality-only validation unchanged (fast mode)",
        "Reports validation errors with puzzle file, index, and specific failure reason",
        "All 9 puzzle files (including from-games.json) pass validation with --engine-verify",
        "The 4 known buggy puzzles from old files are fixed or removed by Pipeline 1 regeneration",
        "python -m py_compile scripts/validate_puzzles.py passes",
        "uv run python scripts/validate_puzzles.py exits with code 0",
        "uv run python scripts/validate_puzzles.py --engine-verify exits with code 0"
      ],
      "priority": 40,
      "passes": false,
      "notes": "Depends on US-035 (motif_detector), US-043 (Lichess puzzle replacements), US-044 (validator exemptions), US-045 (from-games.json fix). The --engine-verify mode is slower but catches false checkmates and suboptimal solutions. Blocked until puzzle quality issues are resolved by US-042 through US-046. See ~/.claude/plans/giggly-bouncing-cake.md for full analysis."
    },
    {
      "id": "US-041",
      "title": "Puzzle generation integration tests and documentation",
      "description": "As a developer, I need tests for the puzzle generation system and updated documentation reflecting the new puzzle infrastructure.",
      "acceptanceCriteria": [
        "tests/test_puzzle_generation.py exists with pytest tests",
        "TestMotifIntegration: verifies detect_motif works on puzzles from each regenerated file",
        "TestPuzzleDeduplication: verifies identical FENs are deduped, different move counters are deduped",
        "TestManifest: verifies manifest loads, saves, tracks processed files correctly",
        "TestGameMining: verifies mining a PGN file extracts puzzles (uses real Stockfish, marked @pytest.mark.e2e)",
        "TestPipelineCLI: verifies CLI --pipeline flag routing works",
        "TestLichessImport: verifies Lichess importer produces valid puzzles with correct schema",
        "CLAUDE.md puzzle table updated with all 9 puzzle files and accurate counts",
        "CLAUDE.md has note about uv run python scripts/import_lichess_puzzles.py for Lichess puzzle import",
        "CLAUDE.md has note about generate_puzzles_from_game MCP tool for dynamic expansion",
        "progress.txt updated with puzzle generation system learnings",
        "uv run python -m pytest tests/test_puzzle_generation.py -v passes",
        "uv run python -m pytest tests/ -v passes (no regressions)"
      ],
      "priority": 41,
      "passes": false,
      "notes": "Final story for the puzzle overhaul. Depends on US-035 through US-040, plus US-042 (Lichess importer), US-043 (puzzle replacements), US-044 (validator exemptions), US-045 (from-games fix). See ~/.claude/plans/giggly-bouncing-cake.md for full analysis."
    },
    {
      "id": "US-042",
      "title": "Create Lichess puzzle importer",
      "description": "As a developer, I need a streaming importer that extracts high-quality puzzles from the Lichess puzzle database (data/lichess_db_puzzle.csv.zst, 5.75M puzzles) filtered by theme, rating, and popularity, converting them to chess_rocket puzzle format.",
      "acceptanceCriteria": [
        "scripts/import_lichess_puzzles.py exists with CLI interface",
        "Streams data/lichess_db_puzzle.csv.zst using zstandard (no full file load into memory)",
        "CLI flags: --themes (comma-separated), --min-rating, --max-rating, --min-popularity (default 80), --limit, --output",
        "Correctly handles Lichess move format: applies moves[0] to FEN to get puzzle position, then moves[1:] alternates solution/opponent-response",
        "For our format: puzzle FEN = position after moves[0], solution_moves = full moves[1:] sequence (solution + forced responses)",
        "Theme-to-motif mapping: backRankMate->back_rank_mate, mateIn1/mateIn2/mateIn3->checkmate, smotheredMate/arabianMate/anastasiasMate->checkmate, fork->fork, pin->pin, skewer->skewer, discoveredAttack->discovered_attack, doubleCheck->double_check, promotion->promotion",
        "Rating-to-difficulty mapping: <1200 beginner, 1200-1800 intermediate, 1800+ advanced",
        "difficulty_rating field set to Lichess puzzle rating",
        "Generates explanation from motif templates (e.g., 'Find the back-rank mate! The king is trapped by its own pieces.')",
        "Output schema matches chess_rocket format: {fen, solution_moves (UCI list), solution_san (SAN list), motif, difficulty, difficulty_rating, explanation, source: 'lichess', lichess_id, lichess_themes}",
        "FEN deduplication: normalizes FEN (strips move counters) to avoid duplicates",
        "Validates every imported puzzle: FEN loads in python-chess, all solution moves are legal in sequence",
        "Checkmate puzzles additionally verified: board.is_checkmate() after full solution",
        "Back-rank mate puzzles additionally verified: mated king on rank 1 or 8",
        "zstandard added to pyproject.toml dependencies",
        "python -m py_compile scripts/import_lichess_puzzles.py passes",
        "uv run python scripts/import_lichess_puzzles.py --themes mateIn1 --limit 5 --output /tmp/test.json produces valid puzzle file"
      ],
      "priority": 42,
      "passes": true,
      "notes": "Foundation for replacing broken puzzle files. Lichess CSV columns: PuzzleId, FEN, Moves, Rating, RatingDeviation, Popularity, NbPlays, Themes, GameUrl, OpeningTags. Key insight: Lichess FEN is the game position BEFORE the puzzle setup move - must apply moves[0] first. See ~/.claude/plans/giggly-bouncing-cake.md for full analysis."
    },
    {
      "id": "US-043",
      "title": "Replace checkmate-patterns.json and back-rank.json with Lichess puzzles",
      "description": "As a developer, I need checkmate-patterns.json (0/28 actual checkmates) and back-rank.json (all constructed/repetitive) fully replaced with high-quality real-game puzzles from the Lichess database.",
      "acceptanceCriteria": [
        "puzzles/checkmate-patterns.json replaced with ~30 Lichess puzzles from themes: mateIn1, mateIn2, smotheredMate, arabianMate, anastasiasMate",
        "ALL checkmate puzzles verified: board.is_checkmate() returns True after full solution",
        "Checkmate difficulty mix: ~15 beginner (<1200), ~10 intermediate (1200-1800), ~5 advanced (1800+)",
        "puzzles/back-rank.json replaced with ~30 Lichess puzzles from theme: backRankMate",
        "ALL back-rank puzzles verified: checkmate occurs AND mated king is on rank 1 or 8",
        "Back-rank difficulty mix: ~15 beginner, ~10 intermediate, ~5 advanced",
        "Both files pass uv run python scripts/validate_puzzles.py (fast mode)",
        "Both files pass uv run python scripts/validate_puzzles.py --engine-verify",
        "All puzzles have source: 'lichess' and lichess_id fields",
        "Old constructed/generated puzzles fully removed (not mixed in)"
      ],
      "priority": 43,
      "passes": true,
      "notes": "Critical fix. checkmate-patterns.json currently has 0/28 actual checkmates (all misclassified tactical exchanges from Pipeline 1). back-rank.json works (28/28 mates) but all are trivial constructed rook-to-8th-rank patterns. Depends on US-042."
    },
    {
      "id": "US-044",
      "title": "Add validator exemptions for opening puzzles",
      "description": "As a developer, I need validate_puzzles.py to exempt opening and opening_trap motif puzzles from the engine-best-move check, since book moves and trap refutations are not always engine-preferred.",
      "acceptanceCriteria": [
        "validate_puzzle_engine() in scripts/validate_puzzles.py skips Check 1 (solution is engine's #1 choice) for motif in ('opening', 'opening_trap')",
        "Check 3 (no stalemate in solution line) still runs for opening puzzles",
        "Check 4 (motif classification warning) still runs for opening puzzles",
        "Exemption logged in validation output: shows 'opening exemption' in status line",
        "opening-moves.json (40 puzzles) passes uv run python scripts/validate_puzzles.py --engine-verify",
        "opening-traps.json (30 puzzles) passes uv run python scripts/validate_puzzles.py --engine-verify",
        "All other puzzle files still fully engine-verified (no exemption leakage)",
        "python -m py_compile scripts/validate_puzzles.py passes"
      ],
      "priority": 44,
      "passes": true,
      "notes": "Quick win - unblocks opening puzzle validation. Opening/trap moves test book knowledge, not engine optimality. e.g. 1.d4 is book but engine might prefer 1.e4. Depends on nothing - can be done in parallel with US-042."
    },
    {
      "id": "US-045",
      "title": "Verify and fix from-games.json with engine verification",
      "description": "As a developer, I need from-games.json cleaned up by running engine verification and removing puzzles with incorrect best moves.",
      "acceptanceCriteria": [
        "Run uv run python scripts/validate_puzzles.py --engine-verify on from-games.json",
        "All puzzles that fail engine verification (solution not within 20cp of best) are removed",
        "Fresh verification used (not hardcoded indices [14, 33, 40, 48] from repair script)",
        "Remaining puzzles >= 10 (minimum threshold for EXPECTED_FILES)",
        "from-games.json passes both fast and engine-verify validation modes",
        "Removed puzzle count logged for audit"
      ],
      "priority": 45,
      "passes": true,
      "notes": "from-games.json currently has 52 puzzles with 4 known bad ones. May have more after fresh engine verification. File grows naturally as student plays more games via generate_puzzles_from_game MCP tool. Depends on US-044 (needs working validator)."
    },
    {
      "id": "US-046",
      "title": "SRS card to puzzle pipeline",
      "description": "As a tutor, I need SRS mistake cards to be convertible into validated puzzles for 'blunder board' review practice, with an MCP tool to export them.",
      "acceptanceCriteria": [
        "SRSManager.export_as_puzzles(min_cp_loss=100) method added to scripts/srs.py",
        "Filters SRS cards where cp_loss >= min_cp_loss",
        "Each card converted to chess_rocket puzzle format: {fen, solution_moves: [best_move UCI], solution_san, motif, difficulty, difficulty_rating, explanation, source: 'srs', card_id}",
        "Validates each exported puzzle: FEN loads in python-chess, best_move is legal",
        "Skips cards with invalid/corrupt FEN or illegal best_move (warns, doesn't crash)",
        "New @mcp.tool() srs_to_puzzles(min_cp_loss=100) in mcp-server/server.py",
        "MCP tool returns {puzzles: [...], total_cards, exported_count, skipped_count}",
        "python -m py_compile scripts/srs.py passes",
        "python -m py_compile mcp-server/server.py passes"
      ],
      "priority": 46,
      "passes": false,
      "notes": "Closes the loop: game mistakes -> SRS cards -> validated puzzles -> review practice. The blunder board concept lets students revisit their own mistakes as puzzles. Depends on nothing."
    },
    {
      "id": "US-047",
      "title": "Final puzzle validation - make US-040 and US-041 pass",
      "description": "As a developer, I need all puzzle quality issues resolved so US-040 (enhanced validation) and US-041 (generation tests) can be marked as passing.",
      "acceptanceCriteria": [
        "uv run python scripts/validate_puzzles.py exits with code 0 (fast mode)",
        "uv run python scripts/validate_puzzles.py --engine-verify exits with code 0 (all 9 files pass)",
        "All checkmate-motif puzzles verified: board.is_checkmate() after solution",
        "All back-rank puzzles verified: mate on rank 1 or 8",
        "Opening puzzles pass with exemption (no engine-best check)",
        "from-games.json has no incorrect best moves",
        "US-040 'passes' field set to true in prd.json",
        "tests/test_puzzle_generation.py updated for Lichess-sourced puzzles (source='lichess' assertions)",
        "US-041 'passes' field set to true in prd.json",
        "uv run python -m pytest tests/test_puzzle_generation.py -v passes",
        "uv run python -m pytest tests/ -v passes (no regressions)",
        "CLAUDE.md puzzle table updated with accurate puzzle counts and sources"
      ],
      "priority": 47,
      "passes": false,
      "notes": "Final verification story. Depends on US-043, US-044, US-045. See ~/.claude/plans/giggly-bouncing-cake.md for implementation order and full analysis."
    }
  ]
}